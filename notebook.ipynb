{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet simulation logicielle\n",
    "* *Rhouch Oussama*\n",
    "* *Cherki Inssaf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/model based.png\" alt=\"CS\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=100\"\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing preprocessing on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_paths = ['data/data_part1.zip', 'data/data_part2.zip', 'data/data_part3.zip', 'data/data_part4.zip']\n",
    "\n",
    "for zip_file_path in zip_file_paths:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_folder = \"data/noise/\"\n",
    "clean_folder = \"data/clean/dev-clean\"\n",
    "output_folder = \"data/noisy/\"\n",
    "denoising_folder = \"data/denoising/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(denoising_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load clean audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/clean/dev-clean\\\\8842\\\\304647\\\\8842-304647-0009.flac',\n",
       " 'data/clean/dev-clean\\\\8842\\\\304647\\\\8842-304647-0010.flac',\n",
       " 'data/clean/dev-clean\\\\8842\\\\304647\\\\8842-304647-0011.flac',\n",
       " 'data/clean/dev-clean\\\\8842\\\\304647\\\\8842-304647-0012.flac',\n",
       " 'data/clean/dev-clean\\\\8842\\\\304647\\\\8842-304647-0013.flac']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of clean files\n",
    "clean_path = []\n",
    "for folder in os.listdir(clean_folder):\n",
    "    folder_path = os.path.join(clean_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".flac\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    clean_path.append(file_path)\n",
    "\n",
    "clean_path[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load noise audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/noise/babble_16k.wav'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of noise files\n",
    "noise_file = \"\"\n",
    "if os.path.isdir(noise_folder):\n",
    "        for root, dirs, files in os.walk(noise_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    noise_file = os.path.join(root, file)\n",
    "\n",
    "noise_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronDataset(Dataset):\n",
    "    def __init__(self, clean_path, noise_file, target_length=None):\n",
    "        self.clean_files = []\n",
    "        self.noise_files = []\n",
    "        self.noisy_files = []\n",
    "        self.clean_path = clean_path\n",
    "        self.noise_file = noise_file\n",
    "        self.target_length = target_length\n",
    "        self.i = 0\n",
    "        self.prepare_data()\n",
    "\n",
    "    def mix(self, clean_audio_path, noise_file):\n",
    "        s, s_rate = librosa.load(clean_audio_path, sr=16000)\n",
    "        u_audio, _ = librosa.load(noise_file, sr=16000)\n",
    "\n",
    "        # Ensure the clean and noisy audio have the same length\n",
    "        min_length = min(len(s), len(u_audio))\n",
    "        s = s[:min_length]\n",
    "        u_audio = u_audio[:min_length]\n",
    "\n",
    "        s_norm = preprocessing.normalize([s])[0]\n",
    "        u_norm = preprocessing.normalize([u_audio])[0]\n",
    "\n",
    "        rsb = 10 * np.log10(np.sum(s_norm**2) / np.sum(u_norm**2))\n",
    "        alpha = 10**(-rsb / 20)\n",
    "        x = s_norm + alpha * u_norm\n",
    "        x = x.astype(np.float32)\n",
    "        s_norm = s_norm.astype(np.float32)\n",
    "        u_norm = u_norm.astype(np.float32)\n",
    "        \n",
    "        # Assuming output_folder is defined somewhere\n",
    "        output_path = f\"{output_folder}noisy_{self.i}.wav\"\n",
    "        sf.write(output_path, x, s_rate)\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        return torch.tensor(s_norm), torch.tensor(u_norm), torch.tensor(x)\n",
    "\n",
    "    def resize_dataset(self):\n",
    "        max_length = max(len(audio) for audio in self.noisy_files + self.clean_files + self.noise_files)\n",
    "\n",
    "        for i in range(len(self.noisy_files)):\n",
    "            self.noisy_files[i] = self.resize_audio(self.noisy_files[i], max_length)\n",
    "            self.clean_files[i] = self.resize_audio(self.clean_files[i], max_length)\n",
    "            self.noise_files[i] = self.resize_audio(self.noise_files[i], max_length)\n",
    "\n",
    "    def resize_audio(self, audio, target_length):\n",
    "        current_length = len(audio)\n",
    "\n",
    "        if current_length > target_length:\n",
    "            # Trim the audio if it's longer than the target length\n",
    "            audio = audio[:target_length]\n",
    "        elif current_length < target_length:\n",
    "            # Pad the audio if it's shorter than the target length\n",
    "            padding = target_length - current_length\n",
    "            audio = np.pad(audio, (0, padding), 'constant')\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def prepare_data(self):\n",
    "        for clean_audio_path in self.clean_path:\n",
    "            clean, noise, noisy = self.mix(clean_audio_path, self.noise_file)\n",
    "            if len(clean) != len(noisy) and len(clean) != len(noise):\n",
    "                print(\"Error: clean, noisy, and noise audio have different lengths\")\n",
    "            \n",
    "            self.clean_files.append(clean)\n",
    "            self.noise_files.append(noise)\n",
    "            self.noisy_files.append(noisy)\n",
    "\n",
    "        # Resize all the data to have the same length\n",
    "        self.resize_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.noisy_files[idx], self.clean_files[idx], self.noise_files[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets (80% training, 20% test)\n",
    "clean_train, clean_test = train_test_split(clean_path[:60], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PerceptronDataset object for the training set\n",
    "preprocessor_train = PerceptronDataset(clean_train, noise_file)\n",
    "\n",
    "# Create a PerceptronDataset object for the test set\n",
    "preprocessor_test = PerceptronDataset(clean_test, noise_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 12)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessor_train), len(preprocessor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=preprocessor_train, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(dataset=preprocessor_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 470400])\n",
      "torch.Size([1, 470400])\n",
      "torch.Size([1, 470400])\n"
     ]
    }
   ],
   "source": [
    "for x, s, u in train_loader:\n",
    "    print(x.shape)\n",
    "    print(s.shape)\n",
    "    print(u.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [10/48], Loss: 0.1139\n",
      "Epoch [1/50], Batch [20/48], Loss: 0.1073\n",
      "Epoch [1/50], Batch [30/48], Loss: 0.1010\n",
      "Epoch [1/50], Batch [40/48], Loss: 0.0949\n",
      "Epoch [2/50], Batch [10/48], Loss: 0.0846\n",
      "Epoch [2/50], Batch [20/48], Loss: 0.0792\n",
      "Epoch [2/50], Batch [30/48], Loss: 0.0742\n",
      "Epoch [2/50], Batch [40/48], Loss: 0.0693\n",
      "Epoch [3/50], Batch [10/48], Loss: 0.0613\n",
      "Epoch [3/50], Batch [20/48], Loss: 0.0571\n",
      "Epoch [3/50], Batch [30/48], Loss: 0.0532\n",
      "Epoch [3/50], Batch [40/48], Loss: 0.0495\n",
      "Epoch [4/50], Batch [10/48], Loss: 0.0433\n",
      "Epoch [4/50], Batch [20/48], Loss: 0.0401\n",
      "Epoch [4/50], Batch [30/48], Loss: 0.0371\n",
      "Epoch [4/50], Batch [40/48], Loss: 0.0344\n",
      "Epoch [5/50], Batch [10/48], Loss: 0.0298\n",
      "Epoch [5/50], Batch [20/48], Loss: 0.0274\n",
      "Epoch [5/50], Batch [30/48], Loss: 0.0252\n",
      "Epoch [5/50], Batch [40/48], Loss: 0.0232\n",
      "Epoch [6/50], Batch [10/48], Loss: 0.0199\n",
      "Epoch [6/50], Batch [20/48], Loss: 0.0182\n",
      "Epoch [6/50], Batch [30/48], Loss: 0.0166\n",
      "Epoch [6/50], Batch [40/48], Loss: 0.0152\n",
      "Epoch [7/50], Batch [10/48], Loss: 0.0129\n",
      "Epoch [7/50], Batch [20/48], Loss: 0.0117\n",
      "Epoch [7/50], Batch [30/48], Loss: 0.0106\n",
      "Epoch [7/50], Batch [40/48], Loss: 0.0096\n",
      "Epoch [8/50], Batch [10/48], Loss: 0.0081\n",
      "Epoch [8/50], Batch [20/48], Loss: 0.0073\n",
      "Epoch [8/50], Batch [30/48], Loss: 0.0066\n",
      "Epoch [8/50], Batch [40/48], Loss: 0.0059\n",
      "Epoch [9/50], Batch [10/48], Loss: 0.0049\n",
      "Epoch [9/50], Batch [20/48], Loss: 0.0044\n",
      "Epoch [9/50], Batch [30/48], Loss: 0.0039\n",
      "Epoch [9/50], Batch [40/48], Loss: 0.0035\n",
      "Epoch [10/50], Batch [10/48], Loss: 0.0029\n",
      "Epoch [10/50], Batch [20/48], Loss: 0.0025\n",
      "Epoch [10/50], Batch [30/48], Loss: 0.0023\n",
      "Epoch [10/50], Batch [40/48], Loss: 0.0020\n",
      "Epoch [11/50], Batch [10/48], Loss: 0.0016\n",
      "Epoch [11/50], Batch [20/48], Loss: 0.0014\n",
      "Epoch [11/50], Batch [30/48], Loss: 0.0013\n",
      "Epoch [11/50], Batch [40/48], Loss: 0.0011\n",
      "Epoch [12/50], Batch [10/48], Loss: 0.0009\n",
      "Epoch [12/50], Batch [20/48], Loss: 0.0008\n",
      "Epoch [12/50], Batch [30/48], Loss: 0.0007\n",
      "Epoch [12/50], Batch [40/48], Loss: 0.0006\n",
      "Epoch [13/50], Batch [10/48], Loss: 0.0005\n",
      "Epoch [13/50], Batch [20/48], Loss: 0.0004\n",
      "Epoch [13/50], Batch [30/48], Loss: 0.0003\n",
      "Epoch [13/50], Batch [40/48], Loss: 0.0003\n",
      "Epoch [14/50], Batch [10/48], Loss: 0.0002\n",
      "Epoch [14/50], Batch [20/48], Loss: 0.0002\n",
      "Epoch [14/50], Batch [30/48], Loss: 0.0002\n",
      "Epoch [14/50], Batch [40/48], Loss: 0.0001\n",
      "Epoch [15/50], Batch [10/48], Loss: 0.0001\n",
      "Epoch [15/50], Batch [20/48], Loss: 0.0001\n",
      "Epoch [15/50], Batch [30/48], Loss: 0.0001\n",
      "Epoch [15/50], Batch [40/48], Loss: 0.0001\n",
      "Epoch [16/50], Batch [10/48], Loss: 0.0001\n",
      "Epoch [16/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [16/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [16/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [40/48], Loss: 0.0000\n",
      "Average Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplePerceptron, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the simple model\n",
    "simple_perceptron_model = SimplePerceptron()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(simple_perceptron_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "simple_perceptron_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    simple_perceptron_model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (X_noisy, y_clean, _) in enumerate(train_loader):\n",
    "        X_noisy, y_clean = X_noisy.to(device), y_clean.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = simple_perceptron_model(X_noisy.unsqueeze(2))\n",
    "        loss = criterion(outputs, y_clean.unsqueeze(2))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training statistics\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "simple_perceptron_model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X_noisy_test, y_clean_test, _) in enumerate(test_loader):\n",
    "        X_noisy_test, y_clean_test = X_noisy_test.to(device), y_clean_test.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_test = simple_perceptron_model(X_noisy_test.unsqueeze(2))\n",
    "        loss_test = criterion(outputs_test, y_clean_test.unsqueeze(2))\n",
    "\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "# Calculate average test loss\n",
    "average_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f'Average Test Loss: {average_test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_loader.dataset.noisy_files[0].shape[0]\n",
    "hidden_size = 128\n",
    "output_size = train_loader.dataset.clean_files[0].shape[0]\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=1, out_features=64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 2000, 64]             128\n",
      "              ReLU-2             [-1, 2000, 64]               0\n",
      "           Dropout-3             [-1, 2000, 64]               0\n",
      "            Linear-4             [-1, 2000, 64]           4,160\n",
      "              ReLU-5             [-1, 2000, 64]               0\n",
      "           Dropout-6             [-1, 2000, 64]               0\n",
      "            Linear-7              [-1, 2000, 1]              65\n",
      "================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.87\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 5.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (2000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [10/48], Loss: 0.0169\n",
      "Epoch [1/50], Batch [20/48], Loss: 0.0085\n",
      "Epoch [1/50], Batch [30/48], Loss: 0.0049\n",
      "Epoch [1/50], Batch [40/48], Loss: 0.0033\n",
      "Epoch [2/50], Batch [10/48], Loss: 0.0020\n",
      "Epoch [2/50], Batch [20/48], Loss: 0.0016\n",
      "Epoch [2/50], Batch [30/48], Loss: 0.0014\n",
      "Epoch [2/50], Batch [40/48], Loss: 0.0012\n",
      "Epoch [3/50], Batch [10/48], Loss: 0.0009\n",
      "Epoch [3/50], Batch [20/48], Loss: 0.0008\n",
      "Epoch [3/50], Batch [30/48], Loss: 0.0007\n",
      "Epoch [3/50], Batch [40/48], Loss: 0.0006\n",
      "Epoch [4/50], Batch [10/48], Loss: 0.0005\n",
      "Epoch [4/50], Batch [20/48], Loss: 0.0004\n",
      "Epoch [4/50], Batch [30/48], Loss: 0.0004\n",
      "Epoch [4/50], Batch [40/48], Loss: 0.0003\n",
      "Epoch [5/50], Batch [10/48], Loss: 0.0003\n",
      "Epoch [5/50], Batch [20/48], Loss: 0.0002\n",
      "Epoch [5/50], Batch [30/48], Loss: 0.0002\n",
      "Epoch [5/50], Batch [40/48], Loss: 0.0002\n",
      "Epoch [6/50], Batch [10/48], Loss: 0.0002\n",
      "Epoch [6/50], Batch [20/48], Loss: 0.0001\n",
      "Epoch [6/50], Batch [30/48], Loss: 0.0001\n",
      "Epoch [6/50], Batch [40/48], Loss: 0.0001\n",
      "Epoch [7/50], Batch [10/48], Loss: 0.0001\n",
      "Epoch [7/50], Batch [20/48], Loss: 0.0001\n",
      "Epoch [7/50], Batch [30/48], Loss: 0.0001\n",
      "Epoch [7/50], Batch [40/48], Loss: 0.0001\n",
      "Epoch [8/50], Batch [10/48], Loss: 0.0001\n",
      "Epoch [8/50], Batch [20/48], Loss: 0.0001\n",
      "Epoch [8/50], Batch [30/48], Loss: 0.0001\n",
      "Epoch [8/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [9/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [9/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [9/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [9/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [10/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [10/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [10/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [10/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [11/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [11/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [11/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [11/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [12/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [12/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [12/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [12/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [13/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [13/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [13/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [13/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [14/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [14/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [14/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [14/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [15/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [15/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [15/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [15/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [16/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [16/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [16/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [16/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [17/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [18/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [19/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [20/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [21/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [22/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [23/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [24/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [25/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [26/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [27/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [28/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [29/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [30/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [31/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [32/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [33/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [34/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [35/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [36/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [37/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [38/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [39/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [40/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [41/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [42/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [43/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [44/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [45/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [46/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [47/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [48/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [49/50], Batch [40/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [10/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [20/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [30/48], Loss: 0.0000\n",
      "Epoch [50/50], Batch [40/48], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model and optimize it with backpropagation on clean data\n",
    "train_loss = []\n",
    "    \n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (X_noisy, y_clean, _) in enumerate(train_loader):\n",
    "        X_noisy, y_clean = X_noisy.to(device), y_clean.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_noisy.unsqueeze(2))\n",
    "        loss = criterion(outputs, y_clean.unsqueeze(2))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training statistics\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "# Save the trained model if needed\n",
    "torch.save(model.state_dict(), 'complex_perceptron_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n",
      "tensor(2.7611e-06)\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model_epoch_'+str(epoch)+'.pth')\n",
    "print('Training complete')\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(num_epochs), train_loss, label='Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X_noisy_test, y_clean_test, _) in enumerate(test_loader):\n",
    "        X_noisy_test, y_clean_test = X_noisy_test.to(device), y_clean_test.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_test = model(X_noisy_test.unsqueeze(2))\n",
    "        loss_test = criterion(outputs_test, y_clean_test.unsqueeze(2))\n",
    "\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "# Calculate average test loss\n",
    "average_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f'Average Test Loss: {average_test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
