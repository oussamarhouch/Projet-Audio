{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet simulation logicielle\n",
    "* *Rhouch Oussama*\n",
    "* *Cherki Inssaf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/model based.png\" alt=\"CS\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "import random\n",
    "import logging\n",
    "torch.cuda.empty_cache()\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing preprocessing on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_paths = ['data/data_part1.zip', 'data/data_part2.zip', 'data/data_part3.zip', 'data/data_part4.zip']\n",
    "\n",
    "for zip_file_path in zip_file_paths:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_folder = \"data/noise/\"\n",
    "clean_folder = \"data/clean/dev-clean\"\n",
    "output_folder = \"data/noisy/\"\n",
    "denoising_folder = \"data/denoising/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(denoising_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load clean audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of clean files\n",
    "clean_path = []\n",
    "for folder in os.listdir(clean_folder):\n",
    "    folder_path = os.path.join(clean_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".flac\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    clean_path.append(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load noise audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of noise files\n",
    "noise_file = \"\"\n",
    "if os.path.isdir(noise_folder):\n",
    "        for root, dirs, files in os.walk(noise_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    noise_file = os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(RSB):\n",
    "    return np.sqrt(10**(RSB/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noisy(clean_file, noise_file, output_file, RSB_range=(3, 9)):\n",
    "    s = clean_file\n",
    "    u, sr = librosa.load(noise_file, sr=None)\n",
    "    u = np.tile(u, int(np.ceil(len(s) / len(u))))[:len(s)]  # Handle variable audio lengths\n",
    "\n",
    "    s_tf = np.fft.fft(s)\n",
    "    u_tf = np.fft.fft(u)\n",
    "\n",
    "    RSB_value = 6\n",
    "    alpha_value = alpha(RSB_value)\n",
    "\n",
    "    x_tf = s_tf + alpha_value * u_tf\n",
    "\n",
    "    x = np.fft.ifft(x_tf)\n",
    "    x = x.astype(np.float32)\n",
    "    x = x / np.max(np.abs(x))  # Normalization\n",
    "\n",
    "    sf.write(output_file, x, sr)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    audio = librosa.effects.pitch_shift(audio, sr, n_steps=random.uniform(-1, 1))  # Pitch shift augmentation\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.clean_files = []\n",
    "        self.noisy_files = []\n",
    "        self.noise = []\n",
    "        self.original_length = []\n",
    "        self.i = 0\n",
    "        \n",
    "        for clean_file in clean_path[:10]:\n",
    "            s, _ = librosa.load(clean_file, sr=None)\n",
    "            self.clean_files.append(torch.tensor(s))\n",
    "            self.original_length.append(len(s))\n",
    "        \n",
    "        self.max_len = max(self.original_length)\n",
    "        u, _ = librosa.load(noise_file, sr=None)\n",
    "        self.noise = u[:self.max_len]\n",
    "        \n",
    "        for i in range(len(self.clean_files)):\n",
    "            self.clean_files[i] = F.pad(self.clean_files[i], (0, self.max_len - len(self.clean_files[i])))\n",
    "        \n",
    "        for clean_file in self.clean_files:\n",
    "            output_path = f\"{output_folder}noisy_{self.i}.wav\"\n",
    "            x = make_noisy(clean_file, noise_file, output_path)\n",
    "            \n",
    "            self.noisy_files.append(x)\n",
    "            \n",
    "            self.i += 1\n",
    "            \n",
    "        for i in range(len(self.clean_files)):\n",
    "            s = self.clean_files[i]\n",
    "            x = self.noisy_files[i]\n",
    "            \n",
    "            self.clean_files[i] = torch.tensor(np.abs(np.fft.fft(s))).to(\"cpu\")\n",
    "            self.noisy_files[i] = torch.tensor(np.abs(np.fft.fft(x))).to(\"cpu\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.clean_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            noisy, clean = self.noisy_files[idx], self.clean_files[idx]\n",
    "        except IndexError:\n",
    "            logging.error(f\"Index {idx} out of range.\")\n",
    "            raise\n",
    "\n",
    "        return noisy, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeechDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470400"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(dataset.__getitem__(0)[0])\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_size, 1024) # First hidden layer\n",
    "        self.fc2 = nn.Linear(1024, 512)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(512, 256)         # Third hidden layer\n",
    "        self.fc4 = nn.Linear(256, input_size)  # Output layer\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(noisy_fft, clean_fft, eps=1e-8):\n",
    "    # Compute the mask as the ratio of clean to noisy magnitudes\n",
    "    mask = clean_fft / (noisy_fft + eps)\n",
    "    \n",
    "    # Ensure the mask values are between 0 and 1\n",
    "    mask = np.clip(mask, 0, 1)\n",
    "    \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 2.1.0 available.\n",
      "INFO:transformers.modeling_xlnet:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(MLP_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]     481,690,624\n",
      "              ReLU-2                 [-1, 1024]               0\n",
      "            Linear-3                  [-1, 512]         524,800\n",
      "              ReLU-4                  [-1, 512]               0\n",
      "            Linear-5                  [-1, 256]         131,328\n",
      "              ReLU-6                  [-1, 256]               0\n",
      "            Linear-7               [-1, 470400]     120,892,800\n",
      "================================================================\n",
      "Total params: 603,239,552\n",
      "Trainable params: 603,239,552\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.79\n",
      "Forward/backward pass size (MB): 3.62\n",
      "Params size (MB): 2301.18\n",
      "Estimated Total Size (MB): 2306.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(MLP_model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8 (0%)]\tLoss: 13.295698\n",
      "Train Epoch: 1 [0/8 (0%)]\tLoss: 174919.968750\n",
      "Train Epoch: 2 [0/8 (0%)]\tLoss: 13018.593750\n",
      "Train Epoch: 3 [0/8 (0%)]\tLoss: 116.040306\n",
      "Train Epoch: 4 [0/8 (0%)]\tLoss: 11.232634\n",
      "Train Epoch: 5 [0/8 (0%)]\tLoss: 28.368010\n",
      "Train Epoch: 6 [0/8 (0%)]\tLoss: 66.125923\n",
      "Train Epoch: 7 [0/8 (0%)]\tLoss: 0.346594\n",
      "Train Epoch: 8 [0/8 (0%)]\tLoss: 0.329154\n",
      "Train Epoch: 9 [0/8 (0%)]\tLoss: 0.678281\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Replace with the number of epochs you desire\n",
    "log_interval = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (noisy, clean) in enumerate(train_loader):\n",
    "        noisy = noisy.float()\n",
    "        clean = clean.float()\n",
    "        \n",
    "        # Make sure to call zero_grad on the optimizer, not the model\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Forward pass through the network\n",
    "        predicted_mask = MLP_model(noisy)\n",
    "        \n",
    "        # Compute the true mask\n",
    "        mask = compute_mask(noisy, clean)\n",
    "        \n",
    "        # Compute loss using the true mask and the predicted mask\n",
    "        loss = criterion(predicted_mask, mask)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(noisy)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    validation_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for noisy, clean in loader:\n",
    "            noisy = noisy.float()\n",
    "            clean = clean.float()\n",
    "\n",
    "            # Forward pass through the network\n",
    "            predicted_mask = model(noisy)\n",
    "\n",
    "            # Compute the true mask\n",
    "            true_mask = compute_mask(noisy, clean)\n",
    "\n",
    "            # Compute loss using the true mask and the predicted mask\n",
    "            loss = criterion(predicted_mask, true_mask)\n",
    "\n",
    "            # Accumulate the validation loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    # Compute the average loss\n",
    "    validation_loss /= len(loader.dataset)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.216946\n"
     ]
    }
   ],
   "source": [
    "# Perform the validation\n",
    "val_loss = validate(MLP_model, test_loader, criterion)\n",
    "print(f'Validation Loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32000 and 470400x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m     write(output_file, sr, denoised_audio\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m denoised_audio\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 54\u001b[0m \u001b[43mdenoise_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/noisy/noisy_0.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/denoising/denoised_0.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 44\u001b[0m, in \u001b[0;36mdenoise_file\u001b[1;34m(model, input_file, output_file, sr)\u001b[0m\n\u001b[0;32m     42\u001b[0m denoised_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m noisy_chunks:\n\u001b[1;32m---> 44\u001b[0m     denoised_chunks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdenoise_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Combine the denoised chunks into a single audio file\u001b[39;00m\n\u001b[0;32m     47\u001b[0m denoised_audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(denoised_chunks)\n",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m, in \u001b[0;36mdenoise_audio\u001b[1;34m(model, noisy, sr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoise_audio\u001b[39m(model, noisy, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Compute the predicted mask\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     predicted_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Convert the predicted mask from PyTorch Tensor to numpy array and transpose it\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     predicted_mask \u001b[38;5;241m=\u001b[39m predicted_mask\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32000 and 470400x1024)"
     ]
    }
   ],
   "source": [
    "def denoise_audio(model, noisy, sr=16000):\n",
    "    # Compute the predicted mask\n",
    "    noisy = noisy.float()\n",
    "    noisy = F.pad(noisy, (0, self.max_len - len(noisy)))\n",
    "    predicted_mask = model(noisy)\n",
    "\n",
    "    # Convert the predicted mask from PyTorch Tensor to numpy array and transpose it\n",
    "    predicted_mask = predicted_mask.detach().numpy().T\n",
    "\n",
    "    # Apply the mask to the noisy audio data\n",
    "    denoised_audio = noisy * predicted_mask\n",
    "\n",
    "    # Compute the magnitude of the denoised audio\n",
    "    denoised_mag = np.abs(np.fft.fft(denoised_audio))\n",
    "\n",
    "    # Compute the phase of the noisy audio\n",
    "    noisy_phase = np.angle(np.fft.fft(noisy))\n",
    "\n",
    "    # Combine the magnitude and phase\n",
    "    denoised_complex = denoised_mag * np.exp(1j * noisy_phase)\n",
    "\n",
    "    # Compute the inverse FFT and convert the result to a real number\n",
    "    denoised_audio = np.real(np.fft.ifft(denoised_complex))\n",
    "\n",
    "    # Cast to float32\n",
    "    denoised_audio = denoised_audio.astype(np.float32)\n",
    "\n",
    "    # Scale to the range [-1, 1]\n",
    "    denoised_audio /= np.max(np.abs(denoised_audio))\n",
    "\n",
    "    return denoised_audio\n",
    "\n",
    "def denoise_file(model, input_file, output_file, sr=16000):\n",
    "    # Load the noisy audio file\n",
    "    noisy, sr = librosa.load(input_file, sr=sr)\n",
    "\n",
    "    # Pad the noisy audio file so that it is divisible into 2 second chunks\n",
    "    noisy = F.pad(torch.tensor(noisy), (0, 2 * sr - len(noisy) % (2 * sr)))\n",
    "\n",
    "    # Split the noisy audio file into 2 second chunks\n",
    "    noisy_chunks = torch.split(noisy, 2 * sr)\n",
    "\n",
    "    # Denoise each chunk\n",
    "    denoised_chunks = []\n",
    "    for chunk in noisy_chunks:\n",
    "        denoised_chunks.append(denoise_audio(model, chunk, sr=sr))\n",
    "\n",
    "    # Combine the denoised chunks into a single audio file\n",
    "    denoised_audio = torch.cat(denoised_chunks)\n",
    "\n",
    "    # Save the denoised audio file\n",
    "    write(output_file, sr, denoised_audio.numpy())\n",
    "    \n",
    "    return denoised_audio.numpy()\n",
    "\n",
    "denoise_file(MLP_model, \"data/noisy/noisy_0.wav\", \"data/denoising/denoised_0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
